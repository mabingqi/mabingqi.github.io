<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="google-site-verification" content="AVTqiFDR7k1WekV_c6kDAq1RkkWpeE-jo_MFZmhauOY" />
  <title>Bingqi Ma</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
    .highlight {
      color: red;
    }
  </style>
  <link rel="canonical" href="https://mabingqi.github.io/" />

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Bingqi Ma
                  </p>
                  <p>
                    I'm a researcher at an AI venture that specializes in video generation.
                    Before this, I worked at the Base Model R&D Department of SenseTime Research, supervised by <a
                      href="https://songguanglu.github.io/">Guanglu Song</a> and <a href="https://liuyu.us/">Yu Liu</a>.
                    I obtained my Bachelor's degree in 2019 and Master's degree in 2022 from <a
                      href="https://scse.buaa.edu.cn/">School of Computer Science and Engineering</a> at <a
                      href="https://www.buaa.edu.cn/">Beihang University</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:mabingqi.1997@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=rcWQWCoAAAAJ">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/a157801/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/mabingqi.jpg"><img style="width:100%;max-width:100%;object-fit: cover;"
                      alt="profile photo" src="images/mabingqi.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    Currently, my main research focuses on the pretraining and application techniques of video generative
                    models. 
                    Before this, I led a small team responsible for the pretraining research of <a
                      href="https://miaohua.sensetime.com/">SenseMirage</a>, a powerful and artistic image
                    diffusion model.
                    I also have extensive research experience in applying large-scale visual models to downstream tasks, including retrieval, recognition, and perception.
                    <span class="highlight">
                      We are recruiting research interns with a specialization in diffusion models. Welcome to email me for further details.
                    </span>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              
              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/PRG.jpg" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.01787" id="MCG_journal">
                    <span class="papertitle">Pretrained Reversible Generation as Unsupervised Visual Representation
                      Learning</span>
                  </a>
                  <br>
                  Rongkun Xue, Jinouwen Zhang, Yazhe Niu, Dazhong Shen, <strong>Bingqi Ma</strong>, Yu Liu, Jing Yang
                  <br>
                  <em>ICCV</em>, 2025
                  <br>
                  <p>
                    A framework to repurpose pretrained score/flow-based
                    generative models by reversing their generation process to extract unsupervised,
                    hierarchy-selectable features for downstream
                    discriminative tasks.
                  </p>
                </td>
              </tr>
              

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/easyref.png" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.09618" id="MCG_journal">
                    <span class="papertitle">EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via
                      Multimodal LLM</span>
                  </a>
                  <br>
                  Zhuofan Zong, Dongzhi Jiang, <strong>Bingqi Ma</strong>, Guanglu Song, Hao Shao, Dazhong Shen, Yu
                  Liu, Hongsheng Li.
                  <br>
                  <em>ICML</em>, 2025
                  <br>
                  <p>A plug-and-play adaptation method that allows diffusion models to be conditioned on multiple
                    reference images alongside
                    the textual prompt leveraging the image–text comprehension capabilities of multimodal large language
                    models</p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/lidit.png" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.11831" id="MCG_journal">
                    <span class="papertitle">Exploring the Role of Large Language Models in Prompt Encoding for
                      Diffusion Models</span>
                  </a>
                  <br>
                  <strong>Bingqi Ma</strong>, Zhuofan Zong, Guanglu Song, Hongsheng Li, Yu Liu.
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <p>A pioneering study on employing large language models as text encoders for diffusion models.
                    It offers rigorous analyses and practical solutions for effectively leveraging decoder-only large
                    language models in this role,
                    and its methods have been adopted in prominent projects such as <a
                      href="https://aivideo.hunyuan.tencent.com/">Hunyuan-Video</a>.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/mova.png" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2404.13046" id="MCG_journal">
                    <span class="papertitle">MoVA: Adapting Mixture of Vision Experts to Multimodal Context</span>
                  </a>
                  <br>
                  Zhuofan Zong*, <strong>Bingqi Ma*</strong>, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang,
                  Hongsheng Li, Yu Liu. (*equal contribution)
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <p>A multimodal large language model capable of automatically selecting the vision encoder that best
                    matches the task based on the user’s instruction, achieving advanced performance on multiple
                    benchmarks in various tasks.
                  </p>
                </td>
              </tr>



              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/esl.png" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2208.04352" id="MCG_journal">
                    <span class="papertitle">Rethinking Robust Representation Learning Under Fine-grained Noisy
                      Faces</span>
                  </a>
                  <br>
                  <strong>Bingqi Ma</strong>, Guanglu Song, Boxiao Liu, Yu Liu.
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <p>A large-scale face recognition training algorithm that is robust to noisy data and automatically
                    resolves
                    conflicts introduced by fine-grained noise.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/trkp.png" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2204.07964" id="MCG_journal">
                    <span class="papertitle">Target-relevant knowledge preservation for multi-source domain adaptive
                      object detection</span>
                  </a>
                  <br>
                  Jiaxi Wu, Jiaxin Chen, Mengzhe He, Yiru Wang, Bo Li, <strong>Bingqi Ma</strong>, Weihao Gan, Wei Wu,
                  Yali Wang, Di Huang.
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <p>A domain adaptive object detection method for multi-source data, effectively mitigating the issue
                    of knowledge
                    degradation in multi-source fusion scenarios.</p>
                </td>
              </tr>



            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Tech Report & Pre-print Papers</h2>
              </td>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/adt.jpg" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2504.11423" id="MCG_journal">
                    <span class="papertitle">ADT: Tuning Diffusion Models with Adversarial Supervision</span>
                  </a>
                  <br>
                  Dazhong Shen, Guanglu Song, Yi Zhang, <strong>Bingqi Ma</strong>, Lujundong Li, Dongzhi Jiang, Zhuofan
                  Zong, Yu Liu
                  <br>
                  <em>Arxiv</em>, 2025
                  <br>
                  <p>A post-training approach for diffusion models, grounded in adversarial generative training that
                    significantly enhances
                    both distribution alignment and image quality.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/face_swap.png" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2503.22179" id="MCG_journal">
                    <span class="papertitle">High-Fidelity Diffusion Face Swapping with ID-Constrained Facial
                      Conditioning</span>
                  </a>
                  <br>
                  Dailan He, Xiahong Wang, Shulun Wang, Guanglu Song, <strong>Bingqi Ma</strong>, Hao Shao, Yu Liu,
                  Hongsheng Li
                  <br>
                  <em>Arxiv</em>, 2025
                  <br>
                  <p>A diffusion model-based image face-swapping method that decouples identity information and
                    attribute information,
                    demonstrating superior identity similarity and attribute consistency.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/mfr.jpg" alt="PontTuset" width="200" style="border-style: none">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2310.16364" id="MCG_journal">
                    <span class="papertitle">Towards Large-scale Masked Face Recognition</span>
                  </a>
                  <br>
                  Manyuan Zhang, <strong>Bingqi Ma</strong>, Guanglu Song, Yunxiao Wang, Hongsheng Li, Yu Liu
                  <br>
                  <em>Arxiv</em>, 2023
                  <br>
                  <p>
                    Top1 solution for the Unconstrained Track of ICCV 2021 MFR Challenge.
                    The work spanned the entire pipeline, from data cleaning and model architecture design to training
                    optimization.
                    Portions of these techniques were contributed to Sensetime FRVT submissions.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table>
            <tbody>
              <tr>
                <h2>Academic Services</h2>
                <p>
                  <li> Reviewer for CVPR, ICCV, ECCV, NeurIPS, AAAI. </li>
                </p>
              </tr>
            </tbody>
          </table>

          <table>
            <tbody>
              <tr>
                <h2>Selected Honors & Awards</h2>
                <p>
                  <li>First-Class Scholarship for Academic Excellence, Beihang University, 2016-2018</li>
                  <li> Outstanding Graduate of Beihang University, 2019 </li>
                  <li> The <strong>1st</strong> place of Glint360K Track and Unconstrained Track in <a
                      href="https://insightface.ai/mfr21">ICCV
                      2021 MFR Challenge.</a></li>
                  <li> The <strong>1st</strong> place of 1:1 Verification Track, 1:N Identification Track and Masked
                    Face
                    Recognition Track in <a href="https://pages.nist.gov/frvt/html/frvt11.html">NIST FRVT</a>, 2021.
                  </li>
                </p>
              </tr>
            </tbody>
          </table>

          <table>
            <tbody>
              <tr>
                <h2>Experience</h2>
                <p>
                  <li>R&D intern at Data-Search, Bytedance, 2018-2019.</li>
                  <li>Research intern at Base-detection, Megvii Research, 2019-2020.</li>
                  <li>Research intern at X-Lab, Sensetime Research, 2020-2021.</li>
                  <li>Research intern at Base Model R&D Department, Sensetime Research, 2021-2022.</li>
                  <li>Researcher at Base Model R&D Department, Sensetime Research, 2022~2025.</li>
                  <li>Researcher at an AI venture, 2025~</li>
                </p>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            The website template was borrowed from
            <a href="https://github.com/jonbarron/jonbarron_website">Jon Baron</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>
